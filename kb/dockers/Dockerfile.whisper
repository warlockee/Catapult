# =============================================================================
# Whisper ASR Docker Image for Model Registry
# =============================================================================
# This Dockerfile creates an inference container for Whisper-based ASR models.
# Supports whisper-base, whisper-large-v3, whisper-large-v3-turbo, etc.
#
# Build Args:
#   MODEL_NAME     - Name of the model
#   BASE_IMAGE     - Base image (default: nvidia/cuda with Python)
#
# Environment Variables (runtime configurable):
#   MODEL_PATH         - Path to model inside container (default: /models/${MODEL_NAME})
#   PORT               - API server port (default: 8000)
#   COMPUTE_TYPE       - Compute type: float16, int8, int8_float16 (default: float16)
#   DEVICE             - Device: cuda, cpu (default: cuda)
#
# Usage:
#   docker build -t whisper-model:v1.0 --build-arg MODEL_NAME=whisper-large-v3 .
#   docker run --gpus all -p 8000:8000 whisper-model:v1.0
# =============================================================================

ARG BASE_IMAGE=nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04
FROM ${BASE_IMAGE}

ARG MODEL_NAME=model

ENV MODEL_NAME=${MODEL_NAME}
ENV MODEL_PATH=/models/${MODEL_NAME}
ENV PORT=8000
ENV COMPUTE_TYPE=float16
ENV DEVICE=cuda
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 python3-pip python3.10-dev ffmpeg libsndfile1 git curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel

WORKDIR /app
RUN mkdir -p /models /tmp/staging

COPY . /tmp/staging/

RUN if [ -d "/tmp/staging/models/${MODEL_NAME}" ]; then \
        mv "/tmp/staging/models/${MODEL_NAME}" "/models/${MODEL_NAME}"; \
    elif [ -d "/tmp/staging/models" ]; then \
        mv /tmp/staging/models/* /models/ 2>/dev/null || true; \
    fi

RUN if [ -f "/tmp/staging/requirements/requirements.txt" ]; then \
        pip install --no-cache-dir -r /tmp/staging/requirements/requirements.txt; \
    fi

RUN pip install --no-cache-dir \
    torch==2.3.0 torchaudio==2.3.0 faster-whisper==1.1.0 openai-whisper \
    transformers>=4.49.0 fastapi uvicorn[standard] python-multipart \
    pydantic numpy soundfile librosa

RUN if [ -d "/tmp/staging/third_party" ]; then \
        cp -r /tmp/staging/third_party/* /app/ 2>/dev/null || true; \
    fi

RUN rm -rf /tmp/staging

# Create the inference server
RUN echo 'import os' > /app/server.py && \
    echo 'import logging' >> /app/server.py && \
    echo 'import tempfile' >> /app/server.py && \
    echo 'from typing import Optional' >> /app/server.py && \
    echo 'import torch' >> /app/server.py && \
    echo 'from fastapi import FastAPI, UploadFile, File, Form' >> /app/server.py && \
    echo 'from fastapi.responses import JSONResponse' >> /app/server.py && \
    echo 'from fastapi.middleware.cors import CORSMiddleware' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")' >> /app/server.py && \
    echo 'logger = logging.getLogger(__name__)' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'app = FastAPI(title="Whisper ASR API", version="1.0.0")' >> /app/server.py && \
    echo 'app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'MODEL_PATH = os.environ.get("MODEL_PATH", "/models/model")' >> /app/server.py && \
    echo 'MODEL_NAME = os.environ.get("MODEL_NAME", "whisper-model")' >> /app/server.py && \
    echo 'COMPUTE_TYPE = os.environ.get("COMPUTE_TYPE", "float16")' >> /app/server.py && \
    echo 'DEVICE = os.environ.get("DEVICE", "cuda" if torch.cuda.is_available() else "cpu")' >> /app/server.py && \
    echo 'model = None' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.on_event("startup")' >> /app/server.py && \
    echo 'async def startup():' >> /app/server.py && \
    echo '    global model' >> /app/server.py && \
    echo '    logger.info(f"Starting Whisper ASR server for {MODEL_NAME}")' >> /app/server.py && \
    echo '    logger.info(f"Model path: {MODEL_PATH}, Device: {DEVICE}")' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        from faster_whisper import WhisperModel' >> /app/server.py && \
    echo '        model = WhisperModel(MODEL_PATH, device=DEVICE, compute_type=COMPUTE_TYPE)' >> /app/server.py && \
    echo '        logger.info("Faster-Whisper model loaded")' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.warning(f"faster-whisper failed: {e}, trying transformers...")' >> /app/server.py && \
    echo '        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor' >> /app/server.py && \
    echo '        model = {"model": AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH, device_map="auto"), "processor": AutoProcessor.from_pretrained(MODEL_PATH)}' >> /app/server.py && \
    echo '    logger.info("Whisper ASR server ready")' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/health")' >> /app/server.py && \
    echo 'async def health():' >> /app/server.py && \
    echo '    return {"status": "healthy", "model": MODEL_NAME, "device": DEVICE}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/")' >> /app/server.py && \
    echo 'async def root():' >> /app/server.py && \
    echo '    return {"message": "Whisper ASR API", "model": MODEL_NAME}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/v1/models")' >> /app/server.py && \
    echo 'async def list_models():' >> /app/server.py && \
    echo '    return {"object": "list", "data": [{"id": MODEL_NAME, "type": "whisper"}]}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.post("/v1/audio/transcriptions")' >> /app/server.py && \
    echo 'async def transcribe(file: UploadFile = File(...), language: Optional[str] = Form(None)):' >> /app/server.py && \
    echo '    if model is None:' >> /app/server.py && \
    echo '        return JSONResponse(status_code=503, content={"error": "Model not loaded"})' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:' >> /app/server.py && \
    echo '            content = await file.read()' >> /app/server.py && \
    echo '            tmp.write(content)' >> /app/server.py && \
    echo '            tmp_path = tmp.name' >> /app/server.py && \
    echo '        if hasattr(model, "transcribe"):' >> /app/server.py && \
    echo '            segments, info = model.transcribe(tmp_path, language=language)' >> /app/server.py && \
    echo '            text = " ".join([s.text for s in segments])' >> /app/server.py && \
    echo '            os.unlink(tmp_path)' >> /app/server.py && \
    echo '            return {"text": text.strip(), "language": info.language}' >> /app/server.py && \
    echo '        else:' >> /app/server.py && \
    echo '            import librosa' >> /app/server.py && \
    echo '            audio, sr = librosa.load(tmp_path, sr=16000)' >> /app/server.py && \
    echo '            inputs = model["processor"](audio, sampling_rate=16000, return_tensors="pt")' >> /app/server.py && \
    echo '            inputs = {k: v.to(model["model"].device) for k, v in inputs.items()}' >> /app/server.py && \
    echo '            with torch.no_grad():' >> /app/server.py && \
    echo '                generated_ids = model["model"].generate(**inputs, max_new_tokens=448)' >> /app/server.py && \
    echo '            text = model["processor"].batch_decode(generated_ids, skip_special_tokens=True)[0]' >> /app/server.py && \
    echo '            os.unlink(tmp_path)' >> /app/server.py && \
    echo '            return {"text": text.strip()}' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.error(f"Transcription error: {e}")' >> /app/server.py && \
    echo '        return JSONResponse(status_code=500, content={"error": str(e)})' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'if __name__ == "__main__":' >> /app/server.py && \
    echo '    import uvicorn' >> /app/server.py && \
    echo '    port = int(os.environ.get("PORT", 8000))' >> /app/server.py && \
    echo '    uvicorn.run(app, host="0.0.0.0", port=port)' >> /app/server.py

EXPOSE ${PORT}

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

CMD ["python", "/app/server.py"]
