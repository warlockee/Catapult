# =============================================================================
# Embedding Model Docker Image for Model Registry
# =============================================================================
# This Dockerfile creates an inference container for embedding models.
# Supports Qwen3-Embedding, sentence-transformers, and similar models.
#
# Build Args:
#   MODEL_NAME     - Name of the model
#   BASE_IMAGE     - Base image (default: nvidia/cuda with Python)
#
# Environment Variables (runtime configurable):
#   MODEL_PATH         - Path to model inside container (default: /models/${MODEL_NAME})
#   PORT               - API server port (default: 8000)
#   MAX_LENGTH         - Maximum sequence length (default: 8192)
#
# Usage:
#   docker build -t embedding:v1.0 --build-arg MODEL_NAME=Qwen3-Embedding-8B .
#   docker run --gpus all -p 8000:8000 embedding:v1.0
# =============================================================================

ARG BASE_IMAGE=nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04
FROM ${BASE_IMAGE}

ARG MODEL_NAME=model

ENV MODEL_NAME=${MODEL_NAME}
ENV MODEL_PATH=/models/${MODEL_NAME}
ENV PORT=8000
ENV MAX_LENGTH=8192
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 python3-pip python3.10-dev git curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel

WORKDIR /app
RUN mkdir -p /models /tmp/staging

COPY . /tmp/staging/

RUN if [ -d "/tmp/staging/models/${MODEL_NAME}" ]; then \
        mv "/tmp/staging/models/${MODEL_NAME}" "/models/${MODEL_NAME}"; \
    elif [ -d "/tmp/staging/models" ]; then \
        mv /tmp/staging/models/* /models/ 2>/dev/null || true; \
    fi

RUN if [ -f "/tmp/staging/requirements/requirements.txt" ]; then \
        pip install --no-cache-dir -r /tmp/staging/requirements/requirements.txt; \
    fi

RUN pip install --no-cache-dir \
    torch==2.3.0 transformers>=4.49.0 sentence-transformers>=3.0.0 \
    fastapi uvicorn[standard] pydantic numpy safetensors accelerate

RUN if [ -d "/tmp/staging/third_party" ]; then \
        cp -r /tmp/staging/third_party/* /app/ 2>/dev/null || true; \
    fi

RUN rm -rf /tmp/staging

# Create the inference server
RUN echo 'import os' > /app/server.py && \
    echo 'import logging' >> /app/server.py && \
    echo 'from typing import List, Optional, Union' >> /app/server.py && \
    echo 'import torch' >> /app/server.py && \
    echo 'import numpy as np' >> /app/server.py && \
    echo 'from fastapi import FastAPI' >> /app/server.py && \
    echo 'from fastapi.responses import JSONResponse' >> /app/server.py && \
    echo 'from fastapi.middleware.cors import CORSMiddleware' >> /app/server.py && \
    echo 'from pydantic import BaseModel' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")' >> /app/server.py && \
    echo 'logger = logging.getLogger(__name__)' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'app = FastAPI(title="Embedding API", version="1.0.0")' >> /app/server.py && \
    echo 'app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'MODEL_PATH = os.environ.get("MODEL_PATH", "/models/model")' >> /app/server.py && \
    echo 'MODEL_NAME = os.environ.get("MODEL_NAME", "embedding-model")' >> /app/server.py && \
    echo 'MAX_LENGTH = int(os.environ.get("MAX_LENGTH", "8192"))' >> /app/server.py && \
    echo 'device = "cuda" if torch.cuda.is_available() else "cpu"' >> /app/server.py && \
    echo 'model = None' >> /app/server.py && \
    echo 'tokenizer = None' >> /app/server.py && \
    echo 'model_type = None' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'class EmbeddingRequest(BaseModel):' >> /app/server.py && \
    echo '    input: Union[str, List[str]]' >> /app/server.py && \
    echo '    model: Optional[str] = None' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.on_event("startup")' >> /app/server.py && \
    echo 'async def startup():' >> /app/server.py && \
    echo '    global model, tokenizer, model_type' >> /app/server.py && \
    echo '    logger.info(f"Starting Embedding server for {MODEL_NAME}")' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        from sentence_transformers import SentenceTransformer' >> /app/server.py && \
    echo '        model = SentenceTransformer(MODEL_PATH, device=device)' >> /app/server.py && \
    echo '        model_type = "sentence_transformers"' >> /app/server.py && \
    echo '        logger.info("SentenceTransformer model loaded")' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.info(f"Trying transformers: {e}")' >> /app/server.py && \
    echo '        from transformers import AutoModel, AutoTokenizer' >> /app/server.py && \
    echo '        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)' >> /app/server.py && \
    echo '        model = AutoModel.from_pretrained(MODEL_PATH, device_map="auto", trust_remote_code=True)' >> /app/server.py && \
    echo '        model_type = "transformers"' >> /app/server.py && \
    echo '        logger.info("Transformers model loaded")' >> /app/server.py && \
    echo '    logger.info("Embedding server ready")' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'def mean_pooling(model_output, attention_mask):' >> /app/server.py && \
    echo '    token_embeddings = model_output[0]' >> /app/server.py && \
    echo '    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()' >> /app/server.py && \
    echo '    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/health")' >> /app/server.py && \
    echo 'async def health():' >> /app/server.py && \
    echo '    return {"status": "healthy", "model": MODEL_NAME, "model_type": model_type}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/")' >> /app/server.py && \
    echo 'async def root():' >> /app/server.py && \
    echo '    return {"message": "Embedding API", "model": MODEL_NAME}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/v1/models")' >> /app/server.py && \
    echo 'async def list_models():' >> /app/server.py && \
    echo '    return {"object": "list", "data": [{"id": MODEL_NAME, "type": "embedding"}]}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.post("/v1/embeddings")' >> /app/server.py && \
    echo 'async def create_embeddings(request: EmbeddingRequest):' >> /app/server.py && \
    echo '    if model is None:' >> /app/server.py && \
    echo '        return JSONResponse(status_code=503, content={"error": "Model not loaded"})' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        texts = [request.input] if isinstance(request.input, str) else request.input' >> /app/server.py && \
    echo '        if model_type == "sentence_transformers":' >> /app/server.py && \
    echo '            embeddings = model.encode(texts, convert_to_numpy=True)' >> /app/server.py && \
    echo '        else:' >> /app/server.py && \
    echo '            encoded = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")' >> /app/server.py && \
    echo '            encoded = {k: v.to(model.device) for k, v in encoded.items()}' >> /app/server.py && \
    echo '            with torch.no_grad():' >> /app/server.py && \
    echo '                outputs = model(**encoded)' >> /app/server.py && \
    echo '                embeddings = mean_pooling(outputs, encoded["attention_mask"])' >> /app/server.py && \
    echo '                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu().numpy()' >> /app/server.py && \
    echo '        data = [{"object": "embedding", "embedding": emb.tolist(), "index": i} for i, emb in enumerate(embeddings)]' >> /app/server.py && \
    echo '        return {"object": "list", "data": data, "model": MODEL_NAME, "usage": {"prompt_tokens": sum(len(t.split()) for t in texts), "total_tokens": sum(len(t.split()) for t in texts)}}' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.error(f"Embedding error: {e}")' >> /app/server.py && \
    echo '        return JSONResponse(status_code=500, content={"error": str(e)})' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'if __name__ == "__main__":' >> /app/server.py && \
    echo '    import uvicorn' >> /app/server.py && \
    echo '    port = int(os.environ.get("PORT", 8000))' >> /app/server.py && \
    echo '    uvicorn.run(app, host="0.0.0.0", port=port)' >> /app/server.py

EXPOSE ${PORT}

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

CMD ["python", "/app/server.py"]
