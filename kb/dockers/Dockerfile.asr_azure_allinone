# syntax=docker/dockerfile:1
# =============================================================================
# ASR All-in-One Docker Image - Azure Base with custom-vllm
# =============================================================================
# Complete ASR service: Audio -> VAD -> 4s Cut -> Inference -> Text
#
# Uses Azure ML foundation-model-inference base with custom-vllm wheel for
# audio model support.
#
# Build:
#   ./build.sh --tag allinone-azure-v2
#
# Run:
#   docker run --gpus '"device=0"' -p 8000:8000 \
#       -v /path/to/model:/models/audio-model-v3-2b-chk-65000:ro \
#       audio-model-v3-2b-svad:allinone-azure-v2
#
# =============================================================================

FROM mcr.microsoft.com/azureml/curated/foundation-model-inference:latest

# Azure-specific labels
LABEL maintainer="Catapult"
LABEL com.azure.ml.framework="vllm"
LABEL com.azure.ml.framework.version="0.10.2"
LABEL com.azure.ml.model.type="audio-asr"
LABEL description="ASR All-in-One Service for Azure"

# Environment setup
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV TORCH_HOME=/root/.cache/torch
ENV MODEL_NAME=audio-model-v3-2b-chk-65000

# Install system dependencies including Python 3.12
RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common \
    git \
    curl \
    ffmpeg \
    libsndfile1 \
    && add-apt-repository -y ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-venv \
    python3.12-dev \
    && rm -rf /var/lib/apt/lists/*

# Patch all system-level CVEs (FFmpeg, zvbi, glibc, GLib, Python, etc.)
RUN apt-get update && apt-get upgrade -y && rm -rf /var/lib/apt/lists/*

# Set up Python 3.12 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 \
    && curl -sS https://bootstrap.pypa.io/get-pip.py | python3.12

# Create and activate venv with Python 3.12
ENV VIRTUAL_ENV=/opt/venv312
RUN python3.12 -m venv $VIRTUAL_ENV
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Upgrade pip
RUN pip install --upgrade pip setuptools wheel

# Install PyTorch and CUDA dependencies first
RUN pip install --no-cache-dir \
    torch==2.5.1 \
    torchvision==0.20.1 \
    torchaudio==2.5.1 \
    --index-url https://download.pytorch.org/whl/cu124

# Copy and install custom-vllm wheel
WORKDIR /tmp/build
COPY vllm-*.whl /tmp/build/
RUN pip install --no-cache-dir /tmp/build/vllm-*.whl && rm -rf /tmp/build

# Pin transformers to match working container (4.57.3)
RUN pip install --no-cache-dir transformers==4.57.3

# Copy audio model files from working custom-vllm (extracted from vllm-0.10.2-allinone-v3)
COPY patches/higgs_audio.py patches/higgs_audio_3.py patches/higgs_audio_config.py \
     patches/higgs_audio_tokenizer.py patches/registry.py \
     /opt/venv312/lib/python3.12/site-packages/vllm/model_executor/models/

# Create a wrapper script to import audio model configs before starting vLLM
RUN cat > /opt/venv312/bin/vllm_higgs_wrapper.py << 'WRAPPEREOF'
#!/usr/bin/env python3
"""Wrapper to register audio model configs before vLLM starts."""
import sys
# Import audio model module to register configs with transformers AutoConfig
from vllm.model_executor.models import higgs_audio
# Now run the actual vLLM api_server
from vllm.entrypoints.openai.api_server import main
if __name__ == "__main__":
    main()
WRAPPEREOF
RUN chmod +x /opt/venv312/bin/vllm_higgs_wrapper.py

# Install additional dependencies for ASR
RUN pip install --no-cache-dir \
    fastapi>=0.100.0 \
    uvicorn>=0.23.0 \
    python-multipart>=0.0.6 \
    soundfile>=0.12.0 \
    packaging>=21.0 \
    whisper-normalizer>=0.1.0 \
    jiwer>=3.0.0 \
    pyarrow>=14.0.0 \
    boto3 \
    s3fs \
    openai>=1.0.0 \
    httpx \
    librosa

# Patch vulnerable pip packages (skip vllm — custom build)
RUN pip install --no-cache-dir --upgrade \
    aiohttp \
    protobuf \
    filelock \
    jaraco.context

# Pre-cache Silero VAD model
RUN python3 -c "import torch; torch.hub.load('snakers4/silero-vad', 'silero_vad', trust_repo=True)" || \
    echo "Warning: Could not pre-cache Silero VAD"

# Pre-cache Whisper model for audio feature extraction (required by audio model)
ENV HF_HOME=/hf_cache
RUN mkdir -p /hf_cache && python3 -c "from huggingface_hub import snapshot_download; snapshot_download('openai/whisper-large-v3-turbo')" || \
    echo "Warning: Could not pre-cache whisper model"

# Enable offline mode to prevent network timeouts during startup
ENV HF_HUB_OFFLINE=1
ENV TRANSFORMERS_OFFLINE=1

# Create app directory
WORKDIR /app/asr

# Write ASR service code
RUN cat > /app/asr/asr_service.py << 'ASREOF'
"""
ASR All-in-One Service - Azure Version

Complete ASR pipeline: Audio -> VAD -> 4s Cut -> Inference -> Text
"""
import asyncio
import base64
import io
import logging
import os
import time
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import Optional

import numpy as np
import soundfile as sf
import torch
import torchaudio
import uvicorn
from fastapi import FastAPI, File, Form, HTTPException, Request, UploadFile
from fastapi.responses import JSONResponse

# Configuration
VLLM_URL = os.getenv("VLLM_URL", "http://localhost:26007/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "asr-model")
SAMPLE_RATE = 16000
CHUNK_SAMPLES = 64000  # 4 seconds at 16kHz
MAX_CHUNKS_PER_REQUEST = 4

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class VADConfig:
    """Silero VAD configuration matching reference implementation."""
    threshold: float = 0.55
    min_speech_duration_ms: int = 125
    min_silence_duration_ms: int = 200
    speech_pad_ms: int = 300


# Global state
vad_model = None
get_speech_timestamps = None
openai_client = None


def load_vad():
    """Load Silero VAD model."""
    global vad_model, get_speech_timestamps
    if vad_model is None:
        logger.info("Loading Silero VAD model...")
        # Use cached path directly to avoid network access
        cache_path = "/root/.cache/torch/hub/snakers4_silero-vad_master"
        model, utils = torch.hub.load(
            repo_or_dir=cache_path,
            model="silero_vad",
            trust_repo=True,
            source='local',
        )
        vad_model = model
        get_speech_timestamps = utils[0]
        logger.info("VAD model loaded")
    return vad_model, get_speech_timestamps


def init_openai_client():
    """Initialize OpenAI client for vLLM."""
    global openai_client
    if openai_client is None:
        import openai
        openai_client = openai.OpenAI(
            base_url=VLLM_URL,
            api_key="not-needed",
        )
        logger.info(f"OpenAI client initialized: {VLLM_URL}")
    return openai_client


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize models on startup."""
    load_vad()
    init_openai_client()
    yield


app = FastAPI(
    title="ASR Service",
    description="All-in-one ASR: Audio -> VAD -> 4s Cut -> Inference -> Text",
    version="1.0.0",
    lifespan=lifespan,
)


def load_audio(audio_bytes: bytes) -> np.ndarray:
    """Load audio from bytes, resample to 16kHz mono."""
    buf = io.BytesIO(audio_bytes)
    try:
        audio, sr = sf.read(buf)
    except Exception:
        buf.seek(0)
        wav, sr = torchaudio.load(buf)
        audio = wav.numpy()
        if len(audio.shape) > 1:
            audio = audio.mean(axis=0)

    if len(audio.shape) > 1:
        audio = audio.mean(axis=1)

    if sr != SAMPLE_RATE:
        wav_tensor = torch.from_numpy(audio).float().unsqueeze(0)
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)
        audio = resampler(wav_tensor).squeeze(0).numpy()

    return audio.astype(np.float32)


def segment_audio(audio: np.ndarray, vad_config: VADConfig) -> list:
    """Run VAD and segment audio into <=4s chunks."""
    model, get_ts = load_vad()

    wav_tensor = torch.from_numpy(audio).float()
    timestamps = get_ts(
        wav_tensor,
        model,
        sampling_rate=SAMPLE_RATE,
        threshold=vad_config.threshold,
        min_speech_duration_ms=vad_config.min_speech_duration_ms,
        min_silence_duration_ms=vad_config.min_silence_duration_ms,
        speech_pad_ms=vad_config.speech_pad_ms,
    )

    # Match reference: if no speech detected, use full audio
    if not timestamps:
        timestamps = [{"start": 0, "end": len(audio)}]

    segments = []
    for ts in timestamps:
        seg = audio[ts["start"]:ts["end"]]
        if len(seg) > CHUNK_SAMPLES:
            for i in range(0, len(seg), CHUNK_SAMPLES):
                chunk = seg[i:i + CHUNK_SAMPLES]
                if len(chunk) > 0:
                    segments.append(chunk)
        else:
            segments.append(seg)

    return segments


def encode_segment(segment: np.ndarray) -> str:
    """Encode audio segment as base64 WAV."""
    buf = io.BytesIO()
    sf.write(buf, segment, SAMPLE_RATE, format="WAV")
    buf.seek(0)
    return base64.b64encode(buf.read()).decode("utf-8")


def _call_vllm(b64_chunks: list, language: str = "English") -> str:
    """Send a batch of audio chunks to vLLM."""
    client = init_openai_client()

    if language.lower() == "auto":
        user_text = "Your task is to listen to audio input and output the exact spoken words as plain text."
    else:
        user_text = f"Your task is to listen to audio input and output the exact spoken words as plain text in {language}."

    content = [{"type": "text", "text": user_text}]
    for i, chunk in enumerate(b64_chunks):
        content.append({
            "type": "audio_url",
            "audio_url": {"url": f"data:audio/wav_{i};base64,{chunk}"}
        })

    messages = [
        {"role": "system", "content": "You are an automatic speech recognition (ASR) system."},
        {"role": "user", "content": content}
    ]

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=messages,
        max_completion_tokens=256,
        temperature=0.0,
        stop=["<|eot_id|>", "<|endoftext|>", "<|audio_eos|>", "<|im_end|>"],
        extra_body={"skip_special_tokens": False, "repetition_penalty": 1.1},
    )

    return response.choices[0].message.content


def transcribe_segments(segments: list, language: str = "English") -> str:
    """Send segments to vLLM and get transcription."""
    if not segments:
        return ""

    b64_chunks = [encode_segment(seg) for seg in segments]

    if len(b64_chunks) <= MAX_CHUNKS_PER_REQUEST:
        return _call_vllm(b64_chunks, language).strip()

    parts = []
    for start in range(0, len(b64_chunks), MAX_CHUNKS_PER_REQUEST):
        batch = b64_chunks[start:start + MAX_CHUNKS_PER_REQUEST]
        text = _call_vllm(batch, language)
        if text:
            parts.append(text.strip())
    return " ".join(parts)


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "model": MODEL_NAME, "vllm_url": VLLM_URL}


@app.get("/")
async def root():
    """Root endpoint for Azure health probes."""
    return {"status": "ok", "service": "asr-model"}


@app.post("/transcribe")
async def transcribe(
    file: UploadFile = File(...),
    language: str = Form(default="English"),
):
    """Transcribe audio file."""
    start_time = time.time()

    try:
        audio_bytes = await file.read()
        audio = load_audio(audio_bytes)
        audio_duration = len(audio) / SAMPLE_RATE

        vad_config = VADConfig()
        segments = segment_audio(audio, vad_config)
        transcription = transcribe_segments(segments, language)

        elapsed = time.time() - start_time

        return JSONResponse({
            "transcription": transcription,
            "language": language,
            "audio_duration_seconds": round(audio_duration, 2),
            "num_segments": len(segments),
            "processing_time_seconds": round(elapsed, 2),
            "realtime_factor": round(audio_duration / elapsed, 2) if elapsed > 0 else 0,
        })

    except Exception as e:
        logger.exception("Transcription failed")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/audio/transcriptions")
async def openai_transcriptions(request: Request):
    """OpenAI Whisper-compatible transcription endpoint.
    Handles both multipart file uploads (real requests) and
    JSON-only requests (Azure MAAP validation test).
    """
    content_type = request.headers.get("content-type", "")

    if "multipart/form-data" in content_type:
        form = await request.form()
        file = form.get("file")
        if file is None:
            return JSONResponse({"text": ""})
        language = form.get("language", "English")
        try:
            audio_bytes = await file.read()
            audio = load_audio(audio_bytes)
            vad_config = VADConfig()
            segments = segment_audio(audio, vad_config)
            transcription = transcribe_segments(segments, language)
            return JSONResponse({"text": transcription})
        except Exception as e:
            logger.exception("Transcription failed")
            raise HTTPException(status_code=500, detail=str(e))
    else:
        # JSON body or empty — Azure MAAP validation sends JSON without file
        return JSONResponse({"text": ""})


@app.post("/transcribe/batch")
async def transcribe_batch(
    files: list = File(...),
    language: str = Form(default="English"),
):
    """Transcribe multiple audio files."""
    results = []
    for f in files:
        try:
            audio_bytes = await f.read()
            audio = load_audio(audio_bytes)
            segments = segment_audio(audio, VADConfig())
            transcription = transcribe_segments(segments, language)
            results.append({
                "filename": f.filename,
                "transcription": transcription,
                "status": "success",
            })
        except Exception as e:
            results.append({
                "filename": f.filename,
                "transcription": None,
                "status": "error",
                "error": str(e),
            })

    return JSONResponse({"results": results})


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ASR Service")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind")
    parser.add_argument("--vllm-url", default="http://localhost:26007/v1", help="vLLM URL")
    parser.add_argument("--model", default="asr-model", help="Model name")
    args = parser.parse_args()

    os.environ["VLLM_URL"] = args.vllm_url
    os.environ["MODEL_NAME"] = args.model

    uvicorn.run(app, host=args.host, port=args.port)
ASREOF

# Write entrypoint script - FIXED: max-model-len 8192 to match h100-31
RUN cat > /app/asr/entrypoint.sh << 'ENTRYEOF'
#!/bin/bash
# ASR All-in-One Entrypoint - Azure Version

set -e

# Defaults - respect environment variables from Azure deployment template
MODEL_PATH="${AZUREML_MODEL_DIR:-}"
MODEL_NAME="${AZUREML_MODEL_NAME:-asr-model}"
VLLM_PORT="${VLLM_PORT:-26007}"
API_PORT="${API_PORT:-8000}"
MAX_MODEL_LEN="${MAX_MODEL_LEN:-8192}"
MAX_NUM_SEQS="${MAX_NUM_SEQS:-174}"
GPU_MEMORY_UTILIZATION="${GPU_MEMORY_UTILIZATION:-0.85}"

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_PATH="$2"
            shift 2
            ;;
        --model-name)
            MODEL_NAME="$2"
            shift 2
            ;;
        --vllm-port)
            VLLM_PORT="$2"
            shift 2
            ;;
        --api-port)
            API_PORT="$2"
            shift 2
            ;;
        --max-model-len)
            MAX_MODEL_LEN="$2"
            shift 2
            ;;
        --max-num-seqs)
            MAX_NUM_SEQS="$2"
            shift 2
            ;;
        --gpu-memory-utilization)
            GPU_MEMORY_UTILIZATION="$2"
            shift 2
            ;;
        -h|--help)
            echo "Usage: entrypoint.sh --model <path> [OPTIONS]"
            echo "Options:"
            echo "  --model PATH              Path to model (required)"
            echo "  --model-name NAME         Served model name (default: asr-model)"
            echo "  --vllm-port PORT          vLLM internal port (default: 26007)"
            echo "  --api-port PORT           ASR API port (default: 8000)"
            echo "  --max-model-len N         Max model length (default: 8192)"
            echo "  --max-num-seqs N          Max num seqs (default: 174)"
            echo "  --gpu-memory-utilization  GPU memory fraction (default: 0.85)"
            exit 0
            ;;
        *)
            echo "Ignoring unknown argument: $1"
            shift
            ;;
    esac
done

# Debug: show environment and filesystem for model discovery
echo "=== Model Discovery Debug ==="
echo "AZUREML_MODEL_DIR=${AZUREML_MODEL_DIR:-<not set>}"
echo "AZUREML_MODEL_NAME=${AZUREML_MODEL_NAME:-<not set>}"
echo "MODEL_PATH=${MODEL_PATH:-<not set>}"
echo "MODEL_NAME=${MODEL_NAME}"
echo "--- /var/azureml-app contents ---"
ls -la /var/azureml-app/ 2>/dev/null || echo "  /var/azureml-app does not exist"
echo "--- /var/azureml-app/azureml-models contents ---"
ls -laR /var/azureml-app/azureml-models/ 2>/dev/null | head -30 || echo "  /var/azureml-app/azureml-models does not exist"
echo "--- /models contents ---"
ls -la /models/ 2>/dev/null || echo "  /models does not exist"
echo "=== End Debug ==="

# If explicit MODEL_PATH set but directory doesn't exist, try to find model
if [ -n "$MODEL_PATH" ] && [ ! -d "$MODEL_PATH" ]; then
    echo "Warning: MODEL_PATH=$MODEL_PATH does not exist, searching for model..."
    MODEL_PATH=""
fi

# Search for model in multiple locations
if [ -z "$MODEL_PATH" ] || [ ! -d "$MODEL_PATH" ]; then
    # Search order: AZUREML mount paths, then /models
    for candidate in \
        "/var/azureml-app/azureml-models/${AZUREML_MODEL_NAME:-audio-asr-model-3}/1" \
        "/var/azureml-app/azureml-models/${AZUREML_MODEL_NAME:-audio-asr-model-3}" \
        "/models/${MODEL_NAME}" \
        "/models/audio-model-v3-2b-chk-65000" \
    ; do
        if [ -d "$candidate" ] && [ -f "$candidate/config.json" ]; then
            MODEL_PATH="$candidate"
            echo "Found model at: $MODEL_PATH"
            break
        fi
    done
fi

# Broader search: any directory containing config.json under azureml-models
if [ -z "$MODEL_PATH" ] || [ ! -d "$MODEL_PATH" ]; then
    for d in /var/azureml-app/azureml-models/*/*/ /var/azureml-app/azureml-models/*/ /var/azureml-app/*/; do
        if [ -d "$d" ] && [ -f "$d/config.json" ]; then
            MODEL_PATH="$d"
            echo "Found model via broad search at: $MODEL_PATH"
            break
        fi
    done
fi

# Last resort: find in /models
if [ -z "$MODEL_PATH" ] || [ ! -d "$MODEL_PATH" ]; then
    MODEL_PATH=$(find /models -mindepth 1 -maxdepth 1 -type d 2>/dev/null | head -1)
fi

if [ -z "$MODEL_PATH" ] || [ ! -d "$MODEL_PATH" ]; then
    echo "Error: Model path not found after searching all known locations."
    echo "Searched: /var/azureml-app/azureml-models/*, /models/*"
    echo "Set AZUREML_MODEL_DIR or use --model <path>"
    exit 1
fi

echo "=============================================="
echo "ASR All-in-One Service (Azure)"
echo "=============================================="
echo "Model: $MODEL_PATH"
echo "Model name: $MODEL_NAME"
echo "vLLM port: $VLLM_PORT (internal)"
echo "API port: $API_PORT (external)"
echo "Max model length: $MAX_MODEL_LEN"
echo "Max num seqs: $MAX_NUM_SEQS"
echo "GPU memory utilization: $GPU_MEMORY_UTILIZATION"
echo "=============================================="

# Start vLLM server in background
# Import audio model first to register config with transformers AutoConfig
echo "Starting vLLM server..."
python3 -c "
from vllm.model_executor.models import higgs_audio
import runpy
import sys
sys.argv = ['vllm', '--served-model-name', '$MODEL_NAME', '--model', '$MODEL_PATH', '--port', '$VLLM_PORT', '--trust-remote-code', '--max-num-seqs', '$MAX_NUM_SEQS', '--max-model-len', '$MAX_MODEL_LEN', '--gpu-memory-utilization', '$GPU_MEMORY_UTILIZATION', '--enable-prefix-caching']
runpy.run_module('vllm.entrypoints.openai.api_server', run_name='__main__')
" &

VLLM_PID=$!

# Wait for vLLM to be ready
echo "Waiting for vLLM to be ready..."
for i in {1..600}; do
    if curl -s "http://localhost:$VLLM_PORT/v1/models" > /dev/null 2>&1; then
        echo "vLLM is ready!"
        break
    fi
    if ! kill -0 $VLLM_PID 2>/dev/null; then
        echo "Error: vLLM process died"
        exit 1
    fi
    sleep 1
done

if ! curl -s "http://localhost:$VLLM_PORT/v1/models" > /dev/null 2>&1; then
    echo "Error: vLLM failed to start within 600 seconds"
    exit 1
fi

# Start ASR service
echo "Starting ASR service on port $API_PORT..."
export VLLM_URL="http://localhost:$VLLM_PORT/v1"
export MODEL_NAME="$MODEL_NAME"

python3 /app/asr/asr_service.py \
    --host 0.0.0.0 \
    --port "$API_PORT" \
    --vllm-url "http://localhost:$VLLM_PORT/v1" \
    --model "$MODEL_NAME"
ENTRYEOF

RUN chmod +x /app/asr/entrypoint.sh

# Create models directory and copy model files (staged by build service)
RUN mkdir -p /models
COPY models/ /models/

# Runtime configuration - FIXED defaults
ENV PORT=8000
ENV VLLM_PORT=26007
ENV GPU_MEMORY_UTIL=0.85
ENV MAX_MODEL_LEN=8192
ENV MAX_NUM_SEQS=174

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

# Expose ports
EXPOSE 8000 26007

WORKDIR /app/asr
ENTRYPOINT ["/app/asr/entrypoint.sh"]
