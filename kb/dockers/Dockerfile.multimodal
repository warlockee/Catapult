# =============================================================================
# Multimodal Docker Image for Model Registry
# =============================================================================
# This Dockerfile creates an inference container for multimodal models.
# Supports Ultravox, Qwen-Omni, and similar audio+text models.
#
# Build Args:
#   MODEL_NAME     - Name of the model
#   BASE_IMAGE     - Base image (default: nvidia/cuda with Python)
#
# Environment Variables (runtime configurable):
#   MODEL_PATH         - Path to model inside container (default: /models/${MODEL_NAME})
#   PORT               - API server port (default: 8000)
#   MAX_MODEL_LEN      - Maximum sequence length (default: 8192)
#
# Usage:
#   docker build -t multimodal:v1.0 --build-arg MODEL_NAME=ultravox-v0_4 .
#   docker run --gpus all -p 8000:8000 multimodal:v1.0
# =============================================================================

ARG BASE_IMAGE=nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04
FROM ${BASE_IMAGE}

ARG MODEL_NAME=model

ENV MODEL_NAME=${MODEL_NAME}
ENV MODEL_PATH=/models/${MODEL_NAME}
ENV PORT=8000
ENV MAX_MODEL_LEN=8192
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 python3-pip python3.10-dev ffmpeg libsndfile1 git curl build-essential \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel

WORKDIR /app
RUN mkdir -p /models /tmp/staging

COPY . /tmp/staging/

RUN if [ -d "/tmp/staging/models/${MODEL_NAME}" ]; then \
        mv "/tmp/staging/models/${MODEL_NAME}" "/models/${MODEL_NAME}"; \
    elif [ -d "/tmp/staging/models" ]; then \
        mv /tmp/staging/models/* /models/ 2>/dev/null || true; \
    fi

RUN if [ -f "/tmp/staging/requirements/requirements.txt" ]; then \
        pip install --no-cache-dir -r /tmp/staging/requirements/requirements.txt; \
    fi

RUN pip install --no-cache-dir \
    torch==2.3.0 torchaudio==2.3.0 torchvision==0.18.0 transformers>=4.49.0 \
    fastapi uvicorn[standard] python-multipart pydantic numpy scipy soundfile librosa \
    pillow einops safetensors accelerate sentencepiece tiktoken

RUN if [ -d "/tmp/staging/third_party" ]; then \
        cp -r /tmp/staging/third_party/* /app/ 2>/dev/null || true; \
    fi

RUN rm -rf /tmp/staging

# Create the inference server
RUN echo 'import os' > /app/server.py && \
    echo 'import logging' >> /app/server.py && \
    echo 'from typing import Optional, List, Union' >> /app/server.py && \
    echo 'import torch' >> /app/server.py && \
    echo 'from fastapi import FastAPI' >> /app/server.py && \
    echo 'from fastapi.responses import JSONResponse' >> /app/server.py && \
    echo 'from fastapi.middleware.cors import CORSMiddleware' >> /app/server.py && \
    echo 'from pydantic import BaseModel' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")' >> /app/server.py && \
    echo 'logger = logging.getLogger(__name__)' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'app = FastAPI(title="Multimodal API", version="1.0.0")' >> /app/server.py && \
    echo 'app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'MODEL_PATH = os.environ.get("MODEL_PATH", "/models/model")' >> /app/server.py && \
    echo 'MODEL_NAME = os.environ.get("MODEL_NAME", "multimodal-model")' >> /app/server.py && \
    echo 'device = "cuda" if torch.cuda.is_available() else "cpu"' >> /app/server.py && \
    echo 'model = None' >> /app/server.py && \
    echo 'processor = None' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'class ChatMessage(BaseModel):' >> /app/server.py && \
    echo '    role: str' >> /app/server.py && \
    echo '    content: Union[str, List[dict]]' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'class ChatRequest(BaseModel):' >> /app/server.py && \
    echo '    messages: List[ChatMessage]' >> /app/server.py && \
    echo '    max_tokens: Optional[int] = 1024' >> /app/server.py && \
    echo '    temperature: Optional[float] = 0.7' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.on_event("startup")' >> /app/server.py && \
    echo 'async def startup():' >> /app/server.py && \
    echo '    global model, processor' >> /app/server.py && \
    echo '    logger.info(f"Starting Multimodal server for {MODEL_NAME}")' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        from transformers import AutoModelForCausalLM, AutoProcessor' >> /app/server.py && \
    echo '        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)' >> /app/server.py && \
    echo '        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)' >> /app/server.py && \
    echo '        logger.info("Multimodal model loaded")' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.error(f"Error loading model: {e}")' >> /app/server.py && \
    echo '    logger.info("Multimodal server ready")' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/health")' >> /app/server.py && \
    echo 'async def health():' >> /app/server.py && \
    echo '    return {"status": "healthy", "model": MODEL_NAME, "device": device}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/")' >> /app/server.py && \
    echo 'async def root():' >> /app/server.py && \
    echo '    return {"message": "Multimodal API", "model": MODEL_NAME}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/v1/models")' >> /app/server.py && \
    echo 'async def list_models():' >> /app/server.py && \
    echo '    return {"object": "list", "data": [{"id": MODEL_NAME, "type": "multimodal"}]}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.post("/v1/chat/completions")' >> /app/server.py && \
    echo 'async def chat_completions(request: ChatRequest):' >> /app/server.py && \
    echo '    if model is None:' >> /app/server.py && \
    echo '        return JSONResponse(status_code=503, content={"error": "Model not loaded"})' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        text_parts = []' >> /app/server.py && \
    echo '        for msg in request.messages:' >> /app/server.py && \
    echo '            if isinstance(msg.content, str):' >> /app/server.py && \
    echo '                text_parts.append(f"{msg.role}: {msg.content}")' >> /app/server.py && \
    echo '        prompt = "\\n".join(text_parts) + "\\nassistant: "' >> /app/server.py && \
    echo '        inputs = processor(text=prompt, return_tensors="pt")' >> /app/server.py && \
    echo '        inputs = {k: v.to(model.device) for k, v in inputs.items()}' >> /app/server.py && \
    echo '        with torch.no_grad():' >> /app/server.py && \
    echo '            outputs = model.generate(**inputs, max_new_tokens=request.max_tokens, do_sample=request.temperature > 0)' >> /app/server.py && \
    echo '        response_text = processor.decode(outputs[0], skip_special_tokens=True)' >> /app/server.py && \
    echo '        if "assistant:" in response_text.lower():' >> /app/server.py && \
    echo '            response_text = response_text.split("assistant:")[-1].strip()' >> /app/server.py && \
    echo '        return {"id": f"chatcmpl-{MODEL_NAME}", "object": "chat.completion", "model": MODEL_NAME, "choices": [{"index": 0, "message": {"role": "assistant", "content": response_text}, "finish_reason": "stop"}]}' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.error(f"Chat error: {e}")' >> /app/server.py && \
    echo '        return JSONResponse(status_code=500, content={"error": str(e)})' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'if __name__ == "__main__":' >> /app/server.py && \
    echo '    import uvicorn' >> /app/server.py && \
    echo '    port = int(os.environ.get("PORT", 8000))' >> /app/server.py && \
    echo '    uvicorn.run(app, host="0.0.0.0", port=port)' >> /app/server.py

EXPOSE ${PORT}

HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

CMD ["python", "/app/server.py"]
