# =============================================================================
# Audio Codec Docker Image for Model Registry
# =============================================================================
# This Dockerfile creates an inference container for audio codec/tokenizer models.
# Supports WavTokenizer, SNAC, EnCodec, DAC, xcodec, etc.
#
# Build Args:
#   MODEL_NAME     - Name of the model
#   BASE_IMAGE     - Base image (default: nvidia/cuda with Python)
#
# Environment Variables (runtime configurable):
#   MODEL_PATH         - Path to model inside container (default: /models/${MODEL_NAME})
#   PORT               - API server port (default: 8000)
#   SAMPLE_RATE        - Audio sample rate (default: 24000)
#
# Usage:
#   docker build -t codec-model:v1.0 --build-arg MODEL_NAME=snac_24khz .
#   docker run --gpus all -p 8000:8000 codec-model:v1.0
# =============================================================================

ARG BASE_IMAGE=nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04
FROM ${BASE_IMAGE}

ARG MODEL_NAME=model

ENV MODEL_NAME=${MODEL_NAME}
ENV MODEL_PATH=/models/${MODEL_NAME}
ENV PORT=8000
ENV SAMPLE_RATE=24000
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 python3-pip python3.10-dev ffmpeg libsndfile1 git curl \
    && rm -rf /var/lib/apt/lists/* \
    && ln -sf /usr/bin/python3.10 /usr/bin/python3 \
    && ln -sf /usr/bin/python3 /usr/bin/python

RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel

WORKDIR /app
RUN mkdir -p /models /tmp/staging

COPY . /tmp/staging/

RUN if [ -d "/tmp/staging/models/${MODEL_NAME}" ]; then \
        mv "/tmp/staging/models/${MODEL_NAME}" "/models/${MODEL_NAME}"; \
    elif [ -d "/tmp/staging/models" ]; then \
        mv /tmp/staging/models/* /models/ 2>/dev/null || true; \
    fi

RUN if [ -f "/tmp/staging/requirements/requirements.txt" ]; then \
        pip install --no-cache-dir -r /tmp/staging/requirements/requirements.txt; \
    fi

RUN pip install --no-cache-dir \
    torch==2.3.0 torchaudio==2.3.0 transformers>=4.49.0 \
    fastapi uvicorn[standard] python-multipart pydantic numpy scipy soundfile librosa einops safetensors

RUN pip install --no-cache-dir snac 2>/dev/null || true
RUN pip install --no-cache-dir encodec 2>/dev/null || true
RUN pip install --no-cache-dir descript-audio-codec 2>/dev/null || true

RUN if [ -d "/tmp/staging/third_party" ]; then \
        cp -r /tmp/staging/third_party/* /app/ 2>/dev/null || true; \
    fi

RUN rm -rf /tmp/staging

# Create the inference server
RUN echo 'import os' > /app/server.py && \
    echo 'import io' >> /app/server.py && \
    echo 'import logging' >> /app/server.py && \
    echo 'from typing import List' >> /app/server.py && \
    echo 'import torch' >> /app/server.py && \
    echo 'import numpy as np' >> /app/server.py && \
    echo 'import soundfile as sf' >> /app/server.py && \
    echo 'from fastapi import FastAPI, UploadFile, File' >> /app/server.py && \
    echo 'from fastapi.responses import Response, JSONResponse' >> /app/server.py && \
    echo 'from fastapi.middleware.cors import CORSMiddleware' >> /app/server.py && \
    echo 'from pydantic import BaseModel' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")' >> /app/server.py && \
    echo 'logger = logging.getLogger(__name__)' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'app = FastAPI(title="Audio Codec API", version="1.0.0")' >> /app/server.py && \
    echo 'app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'MODEL_PATH = os.environ.get("MODEL_PATH", "/models/model")' >> /app/server.py && \
    echo 'MODEL_NAME = os.environ.get("MODEL_NAME", "codec-model")' >> /app/server.py && \
    echo 'SAMPLE_RATE = int(os.environ.get("SAMPLE_RATE", "24000"))' >> /app/server.py && \
    echo 'device = "cuda" if torch.cuda.is_available() else "cpu"' >> /app/server.py && \
    echo 'codec_model = None' >> /app/server.py && \
    echo 'model_type = None' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'class DecodeRequest(BaseModel):' >> /app/server.py && \
    echo '    codes: List[List[int]]' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.on_event("startup")' >> /app/server.py && \
    echo 'async def startup():' >> /app/server.py && \
    echo '    global codec_model, model_type' >> /app/server.py && \
    echo '    logger.info(f"Starting Audio Codec server for {MODEL_NAME}")' >> /app/server.py && \
    echo '    model_name_lower = MODEL_NAME.lower()' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        if "snac" in model_name_lower:' >> /app/server.py && \
    echo '            model_type = "snac"' >> /app/server.py && \
    echo '            from snac import SNAC' >> /app/server.py && \
    echo '            codec_model = SNAC.from_pretrained(MODEL_PATH).eval().to(device)' >> /app/server.py && \
    echo '        elif "encodec" in model_name_lower:' >> /app/server.py && \
    echo '            model_type = "encodec"' >> /app/server.py && \
    echo '            from encodec import EncodecModel' >> /app/server.py && \
    echo '            codec_model = EncodecModel.encodec_model_24khz().to(device)' >> /app/server.py && \
    echo '        else:' >> /app/server.py && \
    echo '            model_type = "generic"' >> /app/server.py && \
    echo '            from transformers import AutoModel' >> /app/server.py && \
    echo '            codec_model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True).to(device)' >> /app/server.py && \
    echo '        logger.info(f"{model_type} codec model loaded")' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.error(f"Error loading model: {e}")' >> /app/server.py && \
    echo '    logger.info("Audio Codec server ready")' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/health")' >> /app/server.py && \
    echo 'async def health():' >> /app/server.py && \
    echo '    return {"status": "healthy", "model": MODEL_NAME, "model_type": model_type}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/")' >> /app/server.py && \
    echo 'async def root():' >> /app/server.py && \
    echo '    return {"message": "Audio Codec API", "model": MODEL_NAME}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.get("/v1/models")' >> /app/server.py && \
    echo 'async def list_models():' >> /app/server.py && \
    echo '    return {"object": "list", "data": [{"id": MODEL_NAME, "type": "codec"}]}' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo '@app.post("/encode")' >> /app/server.py && \
    echo 'async def encode_audio(file: UploadFile = File(...)):' >> /app/server.py && \
    echo '    if codec_model is None:' >> /app/server.py && \
    echo '        return JSONResponse(status_code=503, content={"error": "Model not loaded"})' >> /app/server.py && \
    echo '    try:' >> /app/server.py && \
    echo '        content = await file.read()' >> /app/server.py && \
    echo '        audio, sr = sf.read(io.BytesIO(content))' >> /app/server.py && \
    echo '        if sr != SAMPLE_RATE:' >> /app/server.py && \
    echo '            import librosa' >> /app/server.py && \
    echo '            audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)' >> /app/server.py && \
    echo '        audio_tensor = torch.from_numpy(audio).float().unsqueeze(0).unsqueeze(0).to(device)' >> /app/server.py && \
    echo '        with torch.no_grad():' >> /app/server.py && \
    echo '            if model_type == "snac":' >> /app/server.py && \
    echo '                codes = codec_model.encode(audio_tensor)' >> /app/server.py && \
    echo '                codes_list = [c.cpu().numpy().tolist() for c in codes]' >> /app/server.py && \
    echo '            else:' >> /app/server.py && \
    echo '                codes_list = []' >> /app/server.py && \
    echo '        return {"codes": codes_list, "sample_rate": SAMPLE_RATE}' >> /app/server.py && \
    echo '    except Exception as e:' >> /app/server.py && \
    echo '        logger.error(f"Encode error: {e}")' >> /app/server.py && \
    echo '        return JSONResponse(status_code=500, content={"error": str(e)})' >> /app/server.py && \
    echo '' >> /app/server.py && \
    echo 'if __name__ == "__main__":' >> /app/server.py && \
    echo '    import uvicorn' >> /app/server.py && \
    echo '    port = int(os.environ.get("PORT", 8000))' >> /app/server.py && \
    echo '    uvicorn.run(app, host="0.0.0.0", port=port)' >> /app/server.py

EXPOSE ${PORT}

HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

CMD ["python", "/app/server.py"]
